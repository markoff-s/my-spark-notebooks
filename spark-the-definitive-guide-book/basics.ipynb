{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "flightData2015 = spark\\\n",
    "    .read\\\n",
    "    .option(\"inferSchema\", \"true\")\\\n",
    "    .option(\"header\", \"true\")\\\n",
    "    .csv(\"/tools/Spark-The-Definitive-Guide/data/flight-data/csv/2015-summary.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- DEST_COUNTRY_NAME: string (nullable = true)\n",
      " |-- ORIGIN_COUNTRY_NAME: string (nullable = true)\n",
      " |-- count: integer (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "flightData2015.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(DEST_COUNTRY_NAME='United States', ORIGIN_COUNTRY_NAME='Romania', count=15),\n",
       " Row(DEST_COUNTRY_NAME='United States', ORIGIN_COUNTRY_NAME='Croatia', count=1),\n",
       " Row(DEST_COUNTRY_NAME='United States', ORIGIN_COUNTRY_NAME='Ireland', count=344)]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "flightData2015.take(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(DEST_COUNTRY_NAME='United States', ORIGIN_COUNTRY_NAME='Romania', count=15),\n",
       " Row(DEST_COUNTRY_NAME='United States', ORIGIN_COUNTRY_NAME='Croatia', count=1),\n",
       " Row(DEST_COUNTRY_NAME='United States', ORIGIN_COUNTRY_NAME='Ireland', count=344)]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "flightData2015.take(3)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "== Physical Plan ==\n",
      "*(2) Sort [count#12 ASC NULLS FIRST], true, 0\n",
      "+- Exchange rangepartitioning(count#12 ASC NULLS FIRST, 200)\n",
      "   +- *(1) FileScan csv [DEST_COUNTRY_NAME#10,ORIGIN_COUNTRY_NAME#11,count#12] Batched: false, Format: CSV, Location: InMemoryFileIndex[file:/tools/Spark-The-Definitive-Guide/data/flight-data/csv/2015-summary.csv], PartitionFilters: [], PushedFilters: [], ReadSchema: struct<DEST_COUNTRY_NAME:string,ORIGIN_COUNTRY_NAME:string,count:int>\n"
     ]
    }
   ],
   "source": [
    "flightData2015.sort(\"count\").explain()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "By default, when we perform a shuffle, Spark\n",
    "outputs 200 shuffle partitions. Let’s set this value to 5 to reduce the number of the output\n",
    "partitions from the shuffle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "== Physical Plan ==\n",
      "*(2) Sort [count#12 ASC NULLS FIRST], true, 0\n",
      "+- Exchange rangepartitioning(count#12 ASC NULLS FIRST, 5)\n",
      "   +- *(1) FileScan csv [DEST_COUNTRY_NAME#10,ORIGIN_COUNTRY_NAME#11,count#12] Batched: false, Format: CSV, Location: InMemoryFileIndex[file:/tools/Spark-The-Definitive-Guide/data/flight-data/csv/2015-summary.csv], PartitionFilters: [], PushedFilters: [], ReadSchema: struct<DEST_COUNTRY_NAME:string,ORIGIN_COUNTRY_NAME:string,count:int>\n"
     ]
    }
   ],
   "source": [
    "spark.conf.set(\"spark.sql.shuffle.partitions\", \"5\")\n",
    "\n",
    "flightData2015.sort(\"count\").explain()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------------+-------------------+------+\n",
      "| DEST_COUNTRY_NAME|ORIGIN_COUNTRY_NAME| count|\n",
      "+------------------+-------------------+------+\n",
      "|     United States|      United States|370002|\n",
      "|     United States|             Canada|  8483|\n",
      "|            Canada|      United States|  8399|\n",
      "|     United States|             Mexico|  7187|\n",
      "|            Mexico|      United States|  7140|\n",
      "|    United Kingdom|      United States|  2025|\n",
      "|     United States|     United Kingdom|  1970|\n",
      "|             Japan|      United States|  1548|\n",
      "|     United States|              Japan|  1496|\n",
      "|           Germany|      United States|  1468|\n",
      "|     United States| Dominican Republic|  1420|\n",
      "|Dominican Republic|      United States|  1353|\n",
      "|     United States|            Germany|  1336|\n",
      "|       South Korea|      United States|  1048|\n",
      "|     United States|        The Bahamas|   986|\n",
      "|       The Bahamas|      United States|   955|\n",
      "|     United States|             France|   952|\n",
      "|            France|      United States|   935|\n",
      "|     United States|              China|   920|\n",
      "|          Colombia|      United States|   873|\n",
      "+------------------+-------------------+------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# this took 45 ms\n",
    "spark.conf.set(\"spark.sql.shuffle.partitions\", \"200\")\n",
    "flightData2015.sort(\"count\", ascending=False).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------------+-------------------+------+\n",
      "| DEST_COUNTRY_NAME|ORIGIN_COUNTRY_NAME| count|\n",
      "+------------------+-------------------+------+\n",
      "|     United States|      United States|370002|\n",
      "|     United States|             Canada|  8483|\n",
      "|            Canada|      United States|  8399|\n",
      "|     United States|             Mexico|  7187|\n",
      "|            Mexico|      United States|  7140|\n",
      "|    United Kingdom|      United States|  2025|\n",
      "|     United States|     United Kingdom|  1970|\n",
      "|             Japan|      United States|  1548|\n",
      "|     United States|              Japan|  1496|\n",
      "|           Germany|      United States|  1468|\n",
      "|     United States| Dominican Republic|  1420|\n",
      "|Dominican Republic|      United States|  1353|\n",
      "|     United States|            Germany|  1336|\n",
      "|       South Korea|      United States|  1048|\n",
      "|     United States|        The Bahamas|   986|\n",
      "|       The Bahamas|      United States|   955|\n",
      "|     United States|             France|   952|\n",
      "|            France|      United States|   935|\n",
      "|     United States|              China|   920|\n",
      "|          Colombia|      United States|   873|\n",
      "+------------------+-------------------+------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# this took 32 ms\n",
    "spark.conf.set(\"spark.sql.shuffle.partitions\", \"5\")\n",
    "flightData2015.sort(\"count\", ascending=False).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Both SQL and DataFrame syntax compile to the same execution plan. So, there's no perf penalty in writing sql."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "== Physical Plan ==\n",
      "*(2) HashAggregate(keys=[DEST_COUNTRY_NAME#10], functions=[count(1)])\n",
      "+- Exchange hashpartitioning(DEST_COUNTRY_NAME#10, 5)\n",
      "   +- *(1) HashAggregate(keys=[DEST_COUNTRY_NAME#10], functions=[partial_count(1)])\n",
      "      +- *(1) FileScan csv [DEST_COUNTRY_NAME#10] Batched: false, Format: CSV, Location: InMemoryFileIndex[file:/tools/Spark-The-Definitive-Guide/data/flight-data/csv/2015-summary.csv], PartitionFilters: [], PushedFilters: [], ReadSchema: struct<DEST_COUNTRY_NAME:string>\n",
      "-------------------------------------------\n",
      "== Physical Plan ==\n",
      "*(2) HashAggregate(keys=[DEST_COUNTRY_NAME#10], functions=[count(1)])\n",
      "+- Exchange hashpartitioning(DEST_COUNTRY_NAME#10, 5)\n",
      "   +- *(1) HashAggregate(keys=[DEST_COUNTRY_NAME#10], functions=[partial_count(1)])\n",
      "      +- *(1) FileScan csv [DEST_COUNTRY_NAME#10] Batched: false, Format: CSV, Location: InMemoryFileIndex[file:/tools/Spark-The-Definitive-Guide/data/flight-data/csv/2015-summary.csv], PartitionFilters: [], PushedFilters: [], ReadSchema: struct<DEST_COUNTRY_NAME:string>\n"
     ]
    }
   ],
   "source": [
    "flightData2015.createOrReplaceTempView(\"flight_data_2015\")\n",
    "\n",
    "sqlWay = spark.sql(\"SELECT DEST_COUNTRY_NAME, count(1)\\\n",
    "                    FROM flight_data_2015\\\n",
    "                    GROUP BY DEST_COUNTRY_NAME\")\n",
    "\n",
    "dfWay = flightData2015\\\n",
    "            .groupBy(\"DEST_COUNTRY_NAME\")\\\n",
    "            .count()\n",
    "\n",
    "sqlWay.explain()\n",
    "print(\"-------------------------------------------\")\n",
    "dfWay.explain()\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-------------------------------------------------------------------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "370002"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from pyspark.sql.functions import max\n",
    "\n",
    "flightData2015.select(max(\"count\")).collect()[0][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------+-----------------+\n",
      "|DEST_COUNTRY_NAME|destination_total|\n",
      "+-----------------+-----------------+\n",
      "|    United States|           411352|\n",
      "|           Canada|             8399|\n",
      "|           Mexico|             7140|\n",
      "|   United Kingdom|             2025|\n",
      "|            Japan|             1548|\n",
      "+-----------------+-----------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "maxSql = spark.sql(\"\"\"\n",
    "    SELECT DEST_COUNTRY_NAME, sum(count) as destination_total\n",
    "    FROM flight_data_2015\n",
    "    GROUP BY 1\n",
    "    ORDER BY 2 DESC\n",
    "    LIMIT 5\n",
    "    \"\"\")\n",
    "maxSql.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------+-----------------+\n",
      "|DEST_COUNTRY_NAME|destination_total|\n",
      "+-----------------+-----------------+\n",
      "|    United States|           411352|\n",
      "|           Canada|             8399|\n",
      "|           Mexico|             7140|\n",
      "|   United Kingdom|             2025|\n",
      "|            Japan|             1548|\n",
      "+-----------------+-----------------+\n",
      "\n",
      "+-----------------+-----------------+\n",
      "|DEST_COUNTRY_NAME|destination_total|\n",
      "+-----------------+-----------------+\n",
      "|    United States|           411352|\n",
      "|           Canada|             8399|\n",
      "|           Mexico|             7140|\n",
      "|   United Kingdom|             2025|\n",
      "|            Japan|             1548|\n",
      "+-----------------+-----------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# same as above but with DataFrame syntax\n",
    "from pyspark.sql.functions import desc\n",
    "\n",
    "flightData2015\\\n",
    "    .groupBy(\"DEST_COUNTRY_NAME\")\\\n",
    "    .sum(\"count\")\\\n",
    "    .withColumnRenamed(\"sum(count)\", \"destination_total\")\\\n",
    "    .sort(desc(\"destination_total\"))\\\n",
    "    .limit(5)\\\n",
    "    .show()\n",
    "\n",
    "\n",
    "# OR\n",
    "flightData2015\\\n",
    "    .groupBy(\"DEST_COUNTRY_NAME\")\\\n",
    "    .agg({\"count\": \"sum\"})\\\n",
    "    .withColumnRenamed(\"sum(count)\", \"destination_total\")\\\n",
    "    .orderBy(\"destination_total\", ascending=False)\\\n",
    "    .limit(5)\\\n",
    "    .show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "== Physical Plan ==\n",
      "TakeOrderedAndProject(limit=5, orderBy=[destination_total#466L DESC NULLS LAST], output=[DEST_COUNTRY_NAME#10,destination_total#466L])\n",
      "+- *(2) HashAggregate(keys=[DEST_COUNTRY_NAME#10], functions=[sum(cast(count#12 as bigint))])\n",
      "   +- Exchange hashpartitioning(DEST_COUNTRY_NAME#10, 5)\n",
      "      +- *(1) HashAggregate(keys=[DEST_COUNTRY_NAME#10], functions=[partial_sum(cast(count#12 as bigint))])\n",
      "         +- *(1) FileScan csv [DEST_COUNTRY_NAME#10,count#12] Batched: false, Format: CSV, Location: InMemoryFileIndex[file:/tools/Spark-The-Definitive-Guide/data/flight-data/csv/2015-summary.csv], PartitionFilters: [], PushedFilters: [], ReadSchema: struct<DEST_COUNTRY_NAME:string,count:int>\n"
     ]
    }
   ],
   "source": [
    "# let's check the execution plan\n",
    "from pyspark.sql.functions import desc\n",
    "\n",
    "flightData2015\\\n",
    "    .groupBy(\"DEST_COUNTRY_NAME\")\\\n",
    "    .sum(\"count\")\\\n",
    "    .withColumnRenamed(\"sum(count)\", \"destination_total\")\\\n",
    "    .sort(desc(\"destination_total\"))\\\n",
    "    .limit(5)\\\n",
    "    .explain()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This addition operation happens because Spark will convert an expression written in an input\n",
    "language to Spark’s internal Catalyst representation of that same type information. It then will\n",
    "operate on that internal representation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+-------------+-------------+\n",
      "|number|(number + 10)|(number + 20)|\n",
      "+------+-------------+-------------+\n",
      "|     0|           10|           20|\n",
      "|     1|           11|           21|\n",
      "|     2|           12|           22|\n",
      "|     3|           13|           23|\n",
      "|     4|           14|           24|\n",
      "|     5|           15|           25|\n",
      "|     6|           16|           26|\n",
      "|     7|           17|           27|\n",
      "|     8|           18|           28|\n",
      "|     9|           19|           29|\n",
      "|    10|           20|           30|\n",
      "|    11|           21|           31|\n",
      "|    12|           22|           32|\n",
      "|    13|           23|           33|\n",
      "|    14|           24|           34|\n",
      "|    15|           25|           35|\n",
      "|    16|           26|           36|\n",
      "|    17|           27|           37|\n",
      "|    18|           28|           38|\n",
      "|    19|           29|           39|\n",
      "+------+-------------+-------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "a = 10\n",
    "b = 20\n",
    "df = spark.range(500).toDF(\"number\")\n",
    "xz = df.select(df.number, df.number + a, df[\"number\"] + b)\n",
    "xz.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(id=0), Row(id=1), Row(id=2), Row(id=3), Row(id=4)]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "    rng = spark.range(5)\n",
    "    rng.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "== Physical Plan ==\n",
      "*(1) Range (0, 5, step=1, splits=2)\n"
     ]
    }
   ],
   "source": [
    "rng = spark.range(5).explain()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
