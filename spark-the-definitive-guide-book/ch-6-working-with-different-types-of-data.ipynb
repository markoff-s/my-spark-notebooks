{"cells":[{"cell_type":"code","source":["from pyspark.sql import SparkSession\n\nspark = SparkSession.builder \\\n    .master(\"local\") \\\n    .appName(\"XZ\") \\\n    .config(\"spark.some.config.option\", \"some-value\") \\\n    .getOrCreate()"],"metadata":{},"outputs":[],"execution_count":1},{"cell_type":"code","source":["df = spark\\\n        .read\\\n        .format(\"csv\")\\\n        .option(\"header\", \"true\")\\\n        .option(\"inferSchema\", \"true\")\\\n        .load(\"/FileStore/tables/retail-data/by-day/2010_12_01-*.csv\")\n    \ndf.printSchema()"],"metadata":{},"outputs":[],"execution_count":2},{"cell_type":"code","source":["df.show(5)"],"metadata":{},"outputs":[],"execution_count":3},{"cell_type":"code","source":["df\\\n    .where(df.InvoiceNo == \"536365\")\\\n    .select(\"InvoiceNo\", \"Description\")\\\n    .show(5, False)"],"metadata":{},"outputs":[],"execution_count":4},{"cell_type":"code","source":["df.printSchema()"],"metadata":{},"outputs":[],"execution_count":5},{"cell_type":"code","source":["df\\\n    .where(\"InvoiceNo == 536365\")\\\n    .select(\"InvoiceNo\", \"Description\")\\\n    .show(5, False)"],"metadata":{},"outputs":[],"execution_count":6},{"cell_type":"code","source":["from pyspark.sql.functions import instr, col\n\npriceFilter = col(\"UnitPrice\") > 600\n#descripFilter = instr(df.Description, \"POSTAGE\") >= 1\ndescripFilter = df.Description.contains(\"POSTAGE\")\n\ndf\\\n    .where(df.StockCode.isin(\"DOT\", \"85123A\"))\\\n    .where(priceFilter | descripFilter)\\\n    .show()"],"metadata":{},"outputs":[],"execution_count":7},{"cell_type":"code","source":["from pyspark.sql.functions import instr, col\n\nDOTCodeFilter = col(\"StockCode\") == \"DOT\"\npriceFilter = col(\"UnitPrice\") > 600\ndescripFilter = instr(col(\"Description\"), \"POSTAGE\") >= 1\n\ndf\\\n    .withColumn(\"isExpensive\", DOTCodeFilter & (priceFilter | descripFilter))\\\n    .where(\"isExpensive\")\\\n    .select(\"Description\", \"unitPrice\", \"isExpensive\").show(5)\n"],"metadata":{},"outputs":[],"execution_count":8},{"cell_type":"markdown","source":["### letâ€™s imagine that we found out that we mis-recorded the quantity in our retail dataset and the true quantity is equal to (the current quantity * the unit price) pow of 2 + 5."],"metadata":{}},{"cell_type":"code","source":["from pyspark.sql.functions import pow, ceil\n\n# define a fn\nnewQtyFn = ceil(pow(df.Quantity * df.UnitPrice, 2) + 5)\n\ndf\\\n    .withColumn(\"NewQty\", newQtyFn)\\\n    .select(\"Description\", \"UnitPrice\", \"Quantity\", \"NewQty\", newQtyFn.alias(\"NewerQty\"))\\\n    .show(5, False)\n"],"metadata":{},"outputs":[],"execution_count":10},{"cell_type":"code","source":["# OR as SQL\n\ndf\\\n    .selectExpr(\n        \"CustomerId\",\n        \"(POWER((Quantity * UnitPrice), 2.0) + 5) as realQuantity\")\\\n    .show(2)"],"metadata":{},"outputs":[],"execution_count":11},{"cell_type":"markdown","source":["### Rounding"],"metadata":{}},{"cell_type":"code","source":["from pyspark.sql.functions import lit, round, bround, floor, ceil\n\ndf\\\n    .select(round(lit(2.5253423423), 2),\n           bround(lit(2.52342342342), 3),\n           floor(lit(2.5)),\n           ceil(lit(2.5)))\\\n    .show(1)"],"metadata":{},"outputs":[],"execution_count":13},{"cell_type":"markdown","source":["### Another numerical task is to compute the correlation of two columns"],"metadata":{}},{"cell_type":"code","source":["from pyspark.sql.functions import corr\n\ndf.stat.corr(\"Quantity\", \"UnitPrice\")\ndf.select(corr(\"Quantity\", \"UnitPrice\")).show()"],"metadata":{},"outputs":[],"execution_count":15},{"cell_type":"markdown","source":["### summary statistics"],"metadata":{}},{"cell_type":"code","source":["df\\\n    .describe()\\\n    .select(\"Summary\", \"Quantity\", \"UnitPrice\")\\\n    .show()"],"metadata":{},"outputs":[],"execution_count":17},{"cell_type":"code","source":["colName = \"UnitPrice\"\nquantileProbs = [0.5]\nrelError = 0.05\ndf.stat.approxQuantile(\"UnitPrice\", quantileProbs, relError) # 2.51"],"metadata":{},"outputs":[],"execution_count":18},{"cell_type":"code","source":["df.stat.crosstab(\"StockCode\", \"Quantity\").show(1)"],"metadata":{},"outputs":[],"execution_count":19},{"cell_type":"code","source":["df.stat.freqItems([\"StockCode\", \"Quantity\"]).show()"],"metadata":{},"outputs":[],"execution_count":20},{"cell_type":"code","source":["from pyspark.sql.functions import monotonically_increasing_id\n\ndf.select(monotonically_increasing_id(), \"Description\").show(3, False)"],"metadata":{},"outputs":[],"execution_count":21},{"cell_type":"markdown","source":["### Working with Strings"],"metadata":{}},{"cell_type":"code","source":["from pyspark.sql.functions import lower, upper\n\ndf\\\n    .select(df.Description,\n           lower(df.Description),\n           upper(df.Description))\\\n    .show(2, False)"],"metadata":{},"outputs":[],"execution_count":23},{"cell_type":"code","source":["from pyspark.sql.functions import lit, ltrim, rtrim, rpad, lpad, trim\n\ndf\\\n    .select(\n        ltrim(lit(\" HELLO \")).alias(\"ltrim\"),\n        rtrim(lit(\" HELLO \")).alias(\"rtrim\"),\n        trim(lit(\" HELLO \")).alias(\"trim\"),\n        lpad(lit(\"HELLO\"), 10, \" \").alias(\"lp\"),\n        rpad(lit(\"HELLO\"), 10, \" \").alias(\"rp\"))\\\n    .show(1)"],"metadata":{},"outputs":[],"execution_count":24},{"cell_type":"markdown","source":["### Regular Expressions"],"metadata":{}},{"cell_type":"code","source":["from pyspark.sql.functions import regexp_replace\n\nregex_string = \"BLACK|WHITE|RED|GREEN|BLUE\"\ndf\\\n    .select(\n        regexp_replace(col(\"Description\"), regex_string, \"XZ\").alias(\"color_clean\"),\n        col(\"Description\"))\\\n    .show(5)"],"metadata":{},"outputs":[],"execution_count":26},{"cell_type":"code","source":["from pyspark.sql.functions import translate\n\n# This is done at the character level and will replace all instances of a character with the\n# indexed character in the replacement string\ndf\\\n    .select(translate(col(\"Description\"), \"LEET\", \"1337\"),col(\"Description\"))\\\n    .show(2)"],"metadata":{},"outputs":[],"execution_count":27},{"cell_type":"markdown","source":["### This simple feature can often help you programmatically generate columns or Boolean filters in a way that is simple to understand and extend."],"metadata":{}},{"cell_type":"code","source":["from pyspark.sql.functions import expr, locate\n\nsimpleColors = [\"black\", \"white\", \"red\", \"green\", \"blue\"]\ndef color_locator(column, color_string):\n    return locate(color_string.upper(), column)\\\n        .cast(\"boolean\")\\\n        .alias(\"is_\" + color_string)\n\nselectedColumns = [color_locator(df.Description, c) for c in simpleColors]\n#print(selectedColumns)\nselectedColumns.append(expr(\"*\")) # has to a be Column type\n\ndf\\\n    .select(*selectedColumns)\\\n    .where(expr(\"is_white OR is_red\"))\\\n    .select(\"Description\", *selectedColumns[:5])\\\n    .show(5, False)\n\ndf\\\n    .select(\"Description\",\n            color_locator(df.Description, \"black\"),\n            color_locator(df.Description, \"white\"),\n            color_locator(df.Description, \"red\"))\\\n    .show(5, False)"],"metadata":{},"outputs":[],"execution_count":29},{"cell_type":"markdown","source":["### Working with Dates and Timestamps"],"metadata":{}},{"cell_type":"code","source":["from pyspark.sql.functions import current_date, current_timestamp\n\ndateDF = spark\\\n            .range(5)\\\n            .withColumn(\"today\", current_date())\\\n            .withColumn(\"now\", current_timestamp())\n\ndateDF.show(5, False)"],"metadata":{},"outputs":[],"execution_count":31},{"cell_type":"code","source":["dateDF.printSchema()"],"metadata":{},"outputs":[],"execution_count":32},{"cell_type":"code","source":["from pyspark.sql.functions import date_add, date_sub\n\ndateDF.select(date_sub(col(\"today\"), 5), date_add(col(\"today\"), 5)).show(1)"],"metadata":{},"outputs":[],"execution_count":33},{"cell_type":"code","source":["from pyspark.sql.functions import datediff, months_between, to_date\n\ndateDF\\\n    .withColumn(\"week_ago\", date_sub(col(\"today\"), 7))\\\n    .select(datediff(col(\"week_ago\"), col(\"today\")))\\\n    .show(1)\n\ndateDF\\\n    .select(\n        to_date(lit(\"2016-01-01\")).alias(\"start\"),\n        to_date(lit(\"2017-05-22\")).alias(\"end\"))\\\n    .select(months_between(col(\"start\"), col(\"end\")))\\\n    .show(1)"],"metadata":{},"outputs":[],"execution_count":34},{"cell_type":"code","source":["from pyspark.sql.functions import to_date\n\ndateFormat = \"yyyy-dd-MM\"\ncleanDateDF = spark.range(1).select(\n    to_date(lit(\"2017-12-11\"), dateFormat).alias(\"date\"),\n    to_date(lit(\"2017-20-12\"), dateFormat).alias(\"date2\"))\n\ncleanDateDF.createOrReplaceTempView(\"dateTable2\")\n\ncleanDateDF.show()"],"metadata":{},"outputs":[],"execution_count":35},{"cell_type":"code","source":["cleanDateDF.filter(col(\"date2\") > lit(\"2017-12-12\")).show()"],"metadata":{},"outputs":[],"execution_count":36},{"cell_type":"code","source":["cleanDateDF.filter(col(\"date2\") > \"2017-12-12\").show()"],"metadata":{},"outputs":[],"execution_count":37},{"cell_type":"code","source":["cleanDateDF.filter(cleanDateDF.date2 > \"'2017-12-12'\").show()"],"metadata":{},"outputs":[],"execution_count":38},{"cell_type":"markdown","source":["### Working with Nulls in Data"],"metadata":{}},{"cell_type":"markdown","source":["---\n**WARNING**\n\nTo reiterate, when\nyou define a schema in which all columns are declared to not have null values, Spark will not enforce\nthat and will happily let null values into that column. The nullable signal is simply to help Spark SQL\noptimize for handling that column. If you have null values in columns that should not have null values,\nyou can get an incorrect result or see strange exceptions that can be difficult to debug.\n\n---"],"metadata":{}},{"cell_type":"code","source":["print(f\"Total rows = {df.count()}\")\nprint(f\"Rows w/o ANY na's = {df.dropna('any').count()}\")\nprint(f\"Rows w/o ANY na's (v.2) = {df.na.drop('any').count()}\")\n\nprint(f\"Rows w/o ALL na's = {df.dropna('all').count()}\")\n"],"metadata":{},"outputs":[],"execution_count":41},{"cell_type":"code","source":["# to fill all null values in columns of type String\ndf.na.fill(\"All nulls become this string\").show(2)\n\n"],"metadata":{},"outputs":[],"execution_count":42},{"cell_type":"code","source":["xz = df.exceptAll(df.dropna())\n\n###\nxz.na.fill(\"all\", subset=[\"StockCode\", \"InvoiceNo\"]).show(1)\nxz.na.fill(111, subset=[\"CustomerID\"]).show(1)\nxz.na.fill(0).show(1)\n\n###\ncols_to_fill = {\"CustomerID\":222}\nxz.na.fill(cols_to_fill).show(1)\n"],"metadata":{},"outputs":[],"execution_count":43},{"cell_type":"code","source":["### replace\ndf.na.replace([\"\"], [\"UNKNOWN\"], \"Description\").show(1)\nxz.na.fill(0).replace([0], [333], \"CustomerID\").show(1)"],"metadata":{},"outputs":[],"execution_count":44},{"cell_type":"markdown","source":["### Complex types"],"metadata":{}},{"cell_type":"code","source":["# Structs\nfrom pyspark.sql.functions import struct\n\ncomplexDF = df.select(struct(\"Description\", \"InvoiceNo\").alias(\"complex\"))\ncomplexDF.show(2, False)\n"],"metadata":{},"outputs":[],"execution_count":46},{"cell_type":"code","source":["complexDF.select(\"complex.Description\", \"complex.InvoiceNo\").show(2, False)"],"metadata":{},"outputs":[],"execution_count":47},{"cell_type":"code","source":["complexDF.select(\"complex.*\").show(2, False)"],"metadata":{},"outputs":[],"execution_count":48},{"cell_type":"code","source":["# Arrays\nfrom pyspark.sql.functions import split\n\ndf\\\n  .select(split(df.Description, \" \").alias(\"arr_col\"))\\\n  .show(1, False)\n"],"metadata":{},"outputs":[],"execution_count":49},{"cell_type":"code","source":["df\\\n  .select(split(df.Description, \" \").alias(\"arr_col\"))\\\n  .selectExpr(\"arr_col[0]\")\\\n  .show(3, False)"],"metadata":{},"outputs":[],"execution_count":50},{"cell_type":"code","source":["from pyspark.sql.functions import split, explode\n\ndf\\\n  .withColumn(\"splitted\", split(col(\"Description\"), \" \"))\\\n  .withColumn(\"exploded\", explode(col(\"splitted\")))\\\n  .select(\"Description\", \"splitted\", \"exploded\")\\\n  .show(20, False)\n  \n"],"metadata":{},"outputs":[],"execution_count":51},{"cell_type":"code","source":["from pyspark.sql.functions import split, explode, lit\n\ndf\\\n  .withColumn(\"splitted\", split(lit(\"xz df rt\"), \" \"))\\\n  .withColumn(\"exploded\", explode(col(\"splitted\")))\\\n  .select(\"splitted\", \"exploded\")\\\n  .show(6, False)\n  \n"],"metadata":{},"outputs":[],"execution_count":52},{"cell_type":"code","source":["# Maps\n\nfrom pyspark.sql.functions import create_map\ndf.select(create_map(col(\"Description\"), col(\"InvoiceNo\")).alias(\"complex_map\"))\\\n.show(2, False)\n"],"metadata":{},"outputs":[],"execution_count":53},{"cell_type":"code","source":["df.select(create_map(col(\"Description\"), col(\"InvoiceNo\")).alias(\"complex_map\"))\\\n.selectExpr(\"complex_map['WHITE METAL LANTERN']\").show(2)"],"metadata":{},"outputs":[],"execution_count":54},{"cell_type":"code","source":[""],"metadata":{},"outputs":[],"execution_count":55}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"mimetype":"text/x-python","name":"python","pygments_lexer":"ipython3","codemirror_mode":{"name":"ipython","version":3},"version":"3.7.4","nbconvert_exporter":"python","file_extension":".py"},"name":"ch-6-working-with-different-types-of-data","notebookId":2914909382962744},"nbformat":4,"nbformat_minor":0}
